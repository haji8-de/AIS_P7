{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeudGoTTaGo (듣고따고)\n",
    "오디오 파일을 이용한 악보 전사 서비스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Get Data\n",
    "오디오 파일 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "from pydub import AudioSegment\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_path = \"wav_data\"\n",
    "\n",
    "def download_and_convert_wav():\n",
    "    if not os.path.exists(wav_path): os.mkdir(wav_path)\n",
    "    singer_song = input(\"가수 - 제목: \")\n",
    "    yt = YouTube(input(\"음원 유튜브 url: \"))\n",
    "    yt.streams.filter(only_audio=True).first().download(output_path=wav_path, filename=singer_song+\".mp3\")\n",
    "    AudioSegment.from_file(os.path.join(wav_path, singer_song+\".mp3\")).export(os.path.join(wav_path, singer_song+\".wav\"), format=\"wav\")\n",
    "    os.remove(os.path.join(wav_path, singer_song+\".mp3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Classification\n",
    "### 2-1. 가상악기로 생성한 데이터 분류\n",
    "#### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Dropout, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spt = []\n",
    "ins = []\n",
    "duration_offset = 0\n",
    "# MIDI 표준 128\n",
    "for instrument, note in itertools.product(range(128), range(50)): # 데카르트 곱(cartesian product)\n",
    "    y, sr = librosa.load(\"./GeneralMidi.wav\", sr=None, offset=duration_offset, duration=2.0)\n",
    "    duration_offset += 2\n",
    "    # 데이터 증강을 위해 화이트 노이즈를 섞은 버전도 함께 변환\n",
    "    # 옥타브당 24단계로, 총 7옥타브로 변환\n",
    "    for r in (0, 1e-4, 1e-3):\n",
    "        ret = librosa.cqt(y+((np.random.rand(*y.shape)-0.5)*r if r else 0),\n",
    "                          sr,\n",
    "                          hop_length=1024,\n",
    "                          n_bins=24*7,\n",
    "                          bins_per_octave=24)\n",
    "        # 위상 x, 세기만 관심 있으므로 절대값을 취함\n",
    "        ret = np.abs(ret)\n",
    "        # 스펙토그램 저장\n",
    "        spt.append(ret)\n",
    "        # 악기 번호와 음 높이를 저장\n",
    "        ins.append((instrument, 38+note))\n",
    "# 타악기 46\n",
    "for note in range(46):\n",
    "    y, sr = librosa.load('./GeneralMidi.wav', sr=None, offset=duration_offset, duration=2.0)\n",
    "    duration_offset += 2\n",
    "    for r, s in itertools.product([0, 1e-5, 1e-4, 1e-3], range(7)):\n",
    "        ret = librosa.cqt(y+((np.random.rand(*y.shape) - 0.5)*r*s if r else 0),\n",
    "                          sr,\n",
    "                          hop_length=1024,\n",
    "                          n_bins=24*7,\n",
    "                          bins_per_octave=24)\n",
    "        ret = np.abs(ret)\n",
    "        spt.append(ret)\n",
    "        ins.append((note + 128, 0))\n",
    "    \n",
    "spt = np.array(spt, np.float32)\n",
    "ins = np.array(ins, np.int16)\n",
    "\n",
    "np.savez(\"cqt.npz\", sepc=spt, instr=ins)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), activation=\"relu\", input_shape=(featur.shape[0], feature.shape[1], 1)))\n",
    "model.add(MaxPool2D((2, 2), (2, 2), padding=\"valid\"))\n",
    "model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPool2D((2, 2), (2, 2), padding=\"same\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(174, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. IRMAS Dataset을 이용한 분리\n",
    "#### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import glob\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D, BatchNormalization, LSTM\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Features\n",
    "##### CQT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spt = []\n",
    "\n",
    "for uid in tqdm(train_df.index):\n",
    "    df = train_df.iloc[uid]\n",
    "    path = os.path.join(train_path, df[\"Class\"], df[\"FileName\"])\n",
    "    # load .wav\n",
    "    y, sr = librosa.load(path, sr=None)\n",
    "    ret = librosa.cqt(y, sr)\n",
    "    ret = np.abs(ret)\n",
    "    spt.append(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel = []\n",
    "\n",
    "for uid in tqdm(train_df.index):\n",
    "    df = train_df.iloc[uid]\n",
    "    path = os.path.join(train_path, df[\"Class\"], df[\"FileName\"])\n",
    "    # load .wav\n",
    "    y, sr = librosa.load(path, sr=None)\n",
    "    ret = librosa.feature.mfcc(y, sr)\n",
    "    ret = np.abs(ret)\n",
    "    mel.append(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), activation=\"relu\", input_shape=(feature.shape[0], feature.shape[1], 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPooling2D((2, 2), (2, 2), padding=\"same\"))\n",
    "model.add(Conv2D(128, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D((2, 2), (2, 2), padding=\"valid\"))\n",
    "model.add(Conv2D(256, (3, 3), activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPooling2D((2, 2), (2, 2), padding=\"valid\"))\n",
    "model.add(Conv2D(128, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D((2, 2), (2, 2), padding=\"valid\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(11, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## AMT\n",
    "## 3. Extract Features and Estimate (Model)\n",
    "[# Refer:  https://paperswithcode.com/paper/residual-shuffle-exchange-networks-for-fast#code Aroksak/RSE repo.]\n",
    "### 3-1. Residual Shuffle Exchange Network\n",
    "#### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import signal\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_optimizer\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from musicnet_dataset import MusicNet\n",
    "from musicnet_model import MusicNetModel\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 100\n",
    "EPOCH_SIZE = 2_000\n",
    "EVAL_SIZE = 1_000\n",
    "BATCH_SIZE = 4\n",
    "SMOOTH = 0.1\n",
    "kwargs = {'pin_memory': True}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_weights(n=2048, delta=0.1):\n",
    "    xs = np.linspace(-0.25, 0.75, 2048)\n",
    "    ys = 1 / np.pi * np.arctan(np.sin(2*np.pi*xs) / delta) + 0.5\n",
    "    return torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with MusicNet('../data', train=True, download=True, window=8192, epoch_size=EPOCH_SIZE, pitch_shift=64) as train_dataset, \\\n",
    "     MusicNet('../data', train=False, download=False, window=8192, epoch_size=EVAL_SIZE, pitch_shift=64) as test_dataset:\n",
    "\n",
    "    model = MusicNetModel()\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch_optimizer.RAdam(model.parameters(), lr=0.00125*np.sqrt(0.5))\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction='none', pos_weight=torch.ones([128])*50)\n",
    "    loss_fn.to(device)\n",
    "\n",
    "    weights = get_weights(n=2048, delta=0.5).to(device)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, drop_last=True, **kwargs)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, drop_last=True, **kwargs)\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        t = tqdm(train_loader, total=EPOCH_SIZE // BATCH_SIZE, desc=f\"Train. Epoch {epoch}, loss:\")\n",
    "        losses = []\n",
    "        model.train()\n",
    "        for inputs, targets in t:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.unsqueeze(-2).to(device))\n",
    "            targets = targets[:, ::4, :]\n",
    "            targets = (1 - SMOOTH*2) * targets + SMOOTH\n",
    "            loss = loss_fn(outputs, targets.to(device))\n",
    "            loss = (loss.permute(0, 2, 1) * weights).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            t.set_description(f\"Train. Epoch {epoch}, loss: {np.mean(losses[-100:]):.3f}\")\n",
    "        scheduler.step()\n",
    "\n",
    "        t = tqdm(test_loader, total=EVAL_SIZE // BATCH_SIZE, desc=f\"Validation. Epoch {epoch}.\", leave=False)\n",
    "\n",
    "        all_targets = []\n",
    "        all_preds = []\n",
    "\n",
    "        model.eval()\n",
    "        for inputs, targets in t:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs.unsqueeze(-2).to(device))\n",
    "                outputs = outputs[:, 1024, :].squeeze(1)\n",
    "                targets = targets[:, 4096, :].squeeze(1)\n",
    "                all_targets += list(targets.numpy())\n",
    "                all_preds += list(outputs.detach().cpu().numpy())\n",
    "\n",
    "        targets_np = np.array(all_targets)\n",
    "        preds_np = np.array(all_preds)\n",
    "        mask = targets_np.sum(axis=0) > 0\n",
    "\n",
    "        print(f\"Epoch {epoch}. APS: {average_precision_score(targets_np[:, mask], preds_np[:, mask]) : .2%}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. MT3\n",
    "#### Set Environment and Install Require Library\n",
    "Colab에서하는게 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update -qq && apt-get install -qq libfluidsynth1 build-essential libasound2-dev libjack-dev\n",
    "\n",
    "!pip install nest-asyncio\n",
    "!pip install pyfluidsynth\n",
    "\n",
    "!pip install clu==0.0.7\n",
    "!pip install clu==0.0.7\n",
    "\n",
    "# T5X model\n",
    "!git clone --branch=main https://github.com/google-research/t5x\n",
    "!cd t5x; git reset --hard 2e05ad41778c25521738418de805757bf2e41e9e; cd ..\n",
    "!mv t5x t5x_tmp; mv t5x_tmp/* .; rm -r t5x_tmp\n",
    "!sed -i 's:jax\\[tpu\\]:jax:' setup.py\n",
    "!python3 -m pip install -e .\n",
    "\n",
    "# MT3\n",
    "!git clone --branch=main https://github.com/magenta/mt3\n",
    "!mv mt3 mt3_tmp; mv mt3_tmp/* .; rm -r mt3_tmp\n",
    "!python3 -m pip install -e .\n",
    "\n",
    "!gsutil -q -m cp -r gs://mt3/checkpoints .\n",
    "!gsutil -q -m cp gs://magentadata/soundfonts/SGM-v2.01-Sal-Guit-Bass-V1.3.sf2 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.compat.v2 as tf\n",
    "\n",
    "import functools\n",
    "import gin\n",
    "import jax\n",
    "import librosa\n",
    "import note_seq\n",
    "import seqio\n",
    "import t5\n",
    "import t5x\n",
    "\n",
    "from mt3 import metrics_utils\n",
    "from mt3 import models\n",
    "from mt3 import network\n",
    "from mt3 import note_sequences\n",
    "from mt3 import preprocessors\n",
    "from mt3 import spectrograms\n",
    "from mt3 import vocabularies\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "import nest_asyncio\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "SF2_PATH = 'SGM-v2.01-Sal-Guit-Bass-V1.3.sf2'\n",
    "\n",
    "def upload_audio(sample_rate):\n",
    "  data = list(files.upload().values())\n",
    "  if len(data) > 1:\n",
    "    print('Multiple files uploaded; using only one.')\n",
    "  return note_seq.audio_io.wav_data_to_samples_librosa(\n",
    "    data[0], sample_rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceModel(object):\n",
    "  \"\"\"Wrapper of T5X model for music transcription.\"\"\"\n",
    "  def __init__(self, checkpoint_path, model_type='mt3'):\n",
    "    if model_type == 'ismir2021': # 단일 피아노 모델\n",
    "      num_velocity_bins = 127\n",
    "      self.encoding_spec = note_sequences.NoteEncodingSpec\n",
    "      self.inputs_length = 512\n",
    "    elif model_type == 'mt3': # 다중 악기 모델\n",
    "      num_velocity_bins = 1\n",
    "      self.encoding_spec = note_sequences.NoteEncodingWithTiesSpec\n",
    "      self.inputs_length = 256\n",
    "    else:\n",
    "      raise ValueError('unknown model_type: %s' % model_type)\n",
    "\n",
    "    gin_files = ['/content/mt3/gin/model.gin',\n",
    "                 f'/content/mt3/gin/{model_type}.gin']\n",
    "\n",
    "    self.batch_size = 8\n",
    "    self.outputs_length = 1024\n",
    "    self.sequence_length = {'inputs': self.inputs_length, \n",
    "                            'targets': self.outputs_length}\n",
    "\n",
    "    self.partitioner = t5x.partitioning.PjitPartitioner(\n",
    "        num_partitions=1)\n",
    "\n",
    "    self.spectrogram_config = spectrograms.SpectrogramConfig()\n",
    "    self.codec = vocabularies.build_codec(\n",
    "        vocab_config=vocabularies.VocabularyConfig(\n",
    "            num_velocity_bins=num_velocity_bins))\n",
    "    self.vocabulary = vocabularies.vocabulary_from_codec(self.codec)\n",
    "    self.output_features = {\n",
    "        'inputs': seqio.ContinuousFeature(dtype=tf.float32, rank=2),\n",
    "        'targets': seqio.Feature(vocabulary=self.vocabulary),\n",
    "    }\n",
    "\n",
    "    # Create a T5X model.\n",
    "    self._parse_gin(gin_files)\n",
    "    self.model = self._load_model()\n",
    "\n",
    "    # Restore from checkpoint.\n",
    "    self.restore_from_checkpoint(checkpoint_path)\n",
    "\n",
    "  @property\n",
    "  def input_shapes(self):\n",
    "    return {\n",
    "          'encoder_input_tokens': (self.batch_size, self.inputs_length),\n",
    "          'decoder_input_tokens': (self.batch_size, self.outputs_length)\n",
    "    }\n",
    "\n",
    "  def _parse_gin(self, gin_files):\n",
    "    \"\"\"Parse gin files used to train the model.\"\"\"\n",
    "    gin_bindings = [\n",
    "        'from __gin__ import dynamic_registration',\n",
    "        'from mt3 import vocabularies',\n",
    "        'VOCAB_CONFIG=@vocabularies.VocabularyConfig()',\n",
    "        'vocabularies.VocabularyConfig.num_velocity_bins=%NUM_VELOCITY_BINS'\n",
    "    ]\n",
    "    with gin.unlock_config():\n",
    "      gin.parse_config_files_and_bindings(\n",
    "          gin_files, gin_bindings, finalize_config=False)\n",
    "\n",
    "  def _load_model(self):\n",
    "    model_config = gin.get_configurable(network.T5Config)()\n",
    "    module = network.Transformer(config=model_config)\n",
    "    return models.ContinuousInputsEncoderDecoderModel(\n",
    "        module=module,\n",
    "        input_vocabulary=self.output_features['inputs'].vocabulary,\n",
    "        output_vocabulary=self.output_features['targets'].vocabulary,\n",
    "        optimizer_def=t5x.adafactor.Adafactor(decay_rate=0.8, step_offset=0),\n",
    "        input_depth=spectrograms.input_depth(self.spectrogram_config))\n",
    "\n",
    "  def restore_from_checkpoint(self, checkpoint_path):\n",
    "    train_state_initializer = t5x.utils.TrainStateInitializer(\n",
    "      optimizer_def=self.model.optimizer_def,\n",
    "      init_fn=self.model.get_initial_variables,\n",
    "      input_shapes=self.input_shapes,\n",
    "      partitioner=self.partitioner)\n",
    "\n",
    "    restore_checkpoint_cfg = t5x.utils.RestoreCheckpointConfig(\n",
    "        path=checkpoint_path, mode='specific', dtype='float32')\n",
    "\n",
    "    train_state_axes = train_state_initializer.train_state_axes\n",
    "    self._predict_fn = self._get_predict_fn(train_state_axes)\n",
    "    self._train_state = train_state_initializer.from_checkpoint_or_scratch(\n",
    "        [restore_checkpoint_cfg], init_rng=jax.random.PRNGKey(0))\n",
    "\n",
    "  @functools.lru_cache()\n",
    "  def _get_predict_fn(self, train_state_axes):\n",
    "    def partial_predict_fn(params, batch, decode_rng):\n",
    "      return self.model.predict_batch_with_aux(\n",
    "          params, batch, decoder_params={'decode_rng': None})\n",
    "    return self.partitioner.partition(\n",
    "        partial_predict_fn,\n",
    "        in_axis_resources=(\n",
    "            train_state_axes.params,\n",
    "            t5x.partitioning.PartitionSpec('data',), None),\n",
    "        out_axis_resources=t5x.partitioning.PartitionSpec('data',)\n",
    "    )\n",
    "\n",
    "  def predict_tokens(self, batch, seed=0):\n",
    "    prediction, _ = self._predict_fn(\n",
    "        self._train_state.params, batch, jax.random.PRNGKey(seed))\n",
    "    return self.vocabulary.decode_tf(prediction).numpy()\n",
    "\n",
    "  def __call__(self, audio):\n",
    "    ds = self.audio_to_dataset(audio)\n",
    "    ds = self.preprocess(ds)\n",
    "\n",
    "    model_ds = self.model.FEATURE_CONVERTER_CLS(pack=False)(\n",
    "        ds, task_feature_lengths=self.sequence_length)\n",
    "    model_ds = model_ds.batch(self.batch_size)\n",
    "\n",
    "    inferences = (tokens for batch in model_ds.as_numpy_iterator()\n",
    "                  for tokens in self.predict_tokens(batch))\n",
    "\n",
    "    predictions = []\n",
    "    for example, tokens in zip(ds.as_numpy_iterator(), inferences):\n",
    "      predictions.append(self.postprocess(tokens, example))\n",
    "\n",
    "    result = metrics_utils.event_predictions_to_ns(\n",
    "        predictions, codec=self.codec, encoding_spec=self.encoding_spec)\n",
    "    return result['est_ns']\n",
    "\n",
    "  def audio_to_dataset(self, audio):\n",
    "    frames, frame_times = self._audio_to_frames(audio)\n",
    "    return tf.data.Dataset.from_tensors({\n",
    "        'inputs': frames,\n",
    "        'input_times': frame_times,\n",
    "    })\n",
    "\n",
    "  def _audio_to_frames(self, audio):\n",
    "    frame_size = self.spectrogram_config.hop_width\n",
    "    padding = [0, frame_size - len(audio) % frame_size]\n",
    "    audio = np.pad(audio, padding, mode='constant')\n",
    "    frames = spectrograms.split_audio(audio, self.spectrogram_config)\n",
    "    num_frames = len(audio) // frame_size\n",
    "    times = np.arange(num_frames) / self.spectrogram_config.frames_per_second\n",
    "    return frames, times\n",
    "\n",
    "  def preprocess(self, ds):\n",
    "    pp_chain = [\n",
    "        functools.partial(\n",
    "            t5.data.preprocessors.split_tokens_to_inputs_length,\n",
    "            sequence_length=self.sequence_length,\n",
    "            output_features=self.output_features,\n",
    "            feature_key='inputs',\n",
    "            additional_feature_keys=['input_times']),\n",
    "        preprocessors.add_dummy_targets,\n",
    "        functools.partial(\n",
    "            preprocessors.compute_spectrograms,\n",
    "            spectrogram_config=self.spectrogram_config)\n",
    "    ]\n",
    "    for pp in pp_chain:\n",
    "      ds = pp(ds)\n",
    "    return ds\n",
    "\n",
    "  def postprocess(self, tokens, example):\n",
    "    tokens = self._trim_eos(tokens)\n",
    "    start_time = example['input_times'][0]\n",
    "    start_time -= start_time % (1 / self.codec.steps_per_second)\n",
    "    return {\n",
    "        'est_tokens': tokens,\n",
    "        'start_time': start_time,\n",
    "        'raw_inputs': []\n",
    "    }\n",
    "\n",
    "  @staticmethod\n",
    "  def _trim_eos(tokens):\n",
    "    tokens = np.array(tokens, np.int32)\n",
    "    if vocabularies.DECODED_EOS_ID in tokens:\n",
    "      tokens = tokens[:np.argmax(tokens == vocabularies.DECODED_EOS_ID)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"mt3\" #param[\"ismir2021\", \"mt3\"]\n",
    "\n",
    "checkpoint_path = f'/content/checkpoints/{MODEL}/'\n",
    "\n",
    "inference_model = InferenceModel(checkpoint_path, MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoLab\n",
    "audio = upload_audio(sample_rate=SAMPLE_RATE)\n",
    "\n",
    "note_seq.notebook_utils.colab_play(audio, sample_rate=SAMPLE_RATE)\n",
    "\n",
    "# Local\n",
    "# audio = os.path.join([wav_path, singer_song+\".wav\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_ns = inference_model(audio)\n",
    "\n",
    "note_seq.play_sequence(est_ns, synth=note_seq.fluidsynth, \n",
    "                       sample_rate=SAMPLE_RATE, sf2_path=SF2_PATH)\n",
    "note_seq.plot_sequence(est_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_seq.sequence_proto_to_midi_file(est_ns, '/tmp/transcribed.mid')\n",
    "files.download('/tmp/transcribed.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scoring\n",
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mido import MidiFile, MidiTrack, Message\n",
    "import mido\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "from IPython import *\n",
    "from music21 import *\n",
    "from music21 import converter, instrument, note, chord, stream, midi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!add-apt-repository ppa:mscore-ubuntu/mscore-stable -y\n",
    "!apt-get update\n",
    "!apt-get install musescore\n",
    "\n",
    "!apt-get install xvfb\n",
    "\n",
    "!sh -e /etc/init.d/x11-common start\n",
    "\n",
    "import os\n",
    "os.putenv('DISPLAY', ':99.0')\n",
    "\n",
    "!start-stop-daemon --start --pidfile /var/run/xvfb.pid --make-pidfile --background --exec /usr/bin/Xvfb -- :99 -screen 0 1024x768x24 -ac +extension GLX +render -noreset\n",
    "\n",
    "us = environment.UserSettings()\n",
    "us['musescoreDirectPNGPath'] = '/usr/bin/mscore'\n",
    "us['directoryScratch'] = '/tmp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MELODY_NOTE_OFF = 128\n",
    "MELODY_NO_EVENT = 129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streamToNoteArray(stream):\n",
    "    # Part1. extract from stream\n",
    "    total_length = int(np.round(stream.flat.highestTime/0.25))\n",
    "    stream_list = []\n",
    "    for element in stream.flat:\n",
    "        if isinstance(element, note.Note):\n",
    "            stream_list.append([np.round(element.offset/0.25), np.round(element.quarterLength/0.25), element.pitch.midi])\n",
    "        elif isinstance(element, chord.Chord):\n",
    "            stream_list.append([np.round(element.offset/0.25), np.round(element.quarterLength/0.25), element.sortAscending().pitches[-1].midi])\n",
    "    np_stream_list = np.array(stream_list, dtype=int)\n",
    "    df = pd.DataFrame({'pos': np_stream_list.T[0], 'dur': np_stream_list.T[1], 'pitch': np_stream_list.T[2]})\n",
    "    df = df.sort_values(['pos','pitch'], ascending=[True, False])\n",
    "    df = df.drop_duplicates(subset=['pos'])\n",
    "    # part 2, convert into a sequence of note events\n",
    "    output = np.zeros(total_length+1, dtype=np.int16) + np.int16(MELODY_NO_EVENT) \n",
    "    # Fill in the output list\n",
    "    for i in range(total_length):\n",
    "        if not df[df[\"pos\"]==i].empty:\n",
    "            n = df[df[\"pos\"]==i].iloc[0]\n",
    "            output[i] = n[\"pitch\"]\n",
    "            output[i+n[\"dur\"]] = MELODY_NOTE_OFF\n",
    "    return output\n",
    "def noteArrayToDataFrame(note_array):\n",
    "    df = pd.DataFrame({\"code\": note_array})\n",
    "    df[\"offset\"] = df.index\n",
    "    df[\"duration\"] = df.index\n",
    "    df = df[df[\"code\"]!=MELODY_NO_EVENT]\n",
    "    df[\"duration\"] = df[\"duration\"].diff(-1)*-1*0.25\n",
    "    df = df.fillna(0.25)\n",
    "    return df[[\"code\", \"duration\"]]\n",
    "def noteArrayToStream(note_array):\n",
    "    df = noteArrayToDataFrame(note_array)\n",
    "    melody_stream = stream.Stream()\n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"code\"] == MELODY_NO_EVENT:\n",
    "            new_note = note.Rest() \n",
    "        elif row[\"code\"] == MELODY_NOTE_OFF:\n",
    "            new_note = note.Rest()\n",
    "        else:\n",
    "            new_note = note.Note(row.code)\n",
    "        new_note.quarterLength = row[\"duration\"]\n",
    "        melody_stream.append(new_note)\n",
    "    return melody_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_stream = converter.parse(\"./transcribed.mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 여러 악기들에 대한 악보 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_stream.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 곡의 멜로디 악보 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_stream_rnn = streamToNoteArray(mid_stream)\n",
    "noteArrayToStream(mid_stream).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca7051211f7d0cb84f9a5276aced427e23f2f109de898d0e63a1c129a12d67d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
